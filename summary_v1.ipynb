{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current BPR + LR (linear combination) for CG recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "office = pd.read_csv('/Users/Yiteng/venv/rfm-project/data/SM-office.csv', sep=',', engine='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we take office (OFM) data as one example, where the code segment below (commended) provided the segmentations of the original data towards specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productfile1 = '/Users/Yiteng/venv/rfm-project/data/og_data/ProductMaster.csv'\n",
    "productfile2 = '/Users/Yiteng/venv/rfm-project/data/og_data/ProductMaster_Tops.csv'\n",
    "product1 = pd.read_csv(productfile1, sep=',', engine='c')\n",
    "product2 = pd.read_csv(productfile2, sep=',', engine='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Segmentation according to the meeting with Vijay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# office = sub_segment_df[sub_segment_df.BUID.isin([15,56])]\n",
    "# non_office = sub_segment_df[~sub_segment_df.BUID.isin([15,56])]\n",
    "\n",
    "# sub_segment_df = orders[orders.SKUCode.isin(product1.SKUID)]\n",
    "# sub_segment_df.to_csv('ShoppingMall.csv', index=False)\n",
    "# sub_segment_df2 = orders[orders.SKUCode.isin(product2.SKUID)]\n",
    "# sub_segment_df2.to_csv('Tops_supermarket.csv', index=False)\n",
    "# sub_segment_df3 = orders[~orders.SKUCode.isin(product1.SKUID) & ~orders.SKUCode.isin(product2.SKUID)]\n",
    "# sub_segment_df3.to_csv('Problematic.csv', index=False)\n",
    "\n",
    "# sub_segment_df.TicketNumber.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking \"office\" data for consideration in this notebook, the transaction data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = office\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Personalized Ranking (BPR) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve info from the data -- where we focus on a match between user and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano_bpr import BPR\n",
    "\n",
    "idx = pd.Categorical(list(data.CustomerID)).codes\n",
    "itm = pd.Categorical(list(data.SKUCode)).codes\n",
    "\n",
    "# match the ID and the \"series number\"\n",
    "idx_match = zip(data.CustomerID, idx)\n",
    "itm_match = zip(data.SKUCode, itm)\n",
    "\n",
    "train_set = zip(idx,itm)\n",
    "\n",
    "bpr = BPR(20, idx.max()+1, itm.max()+1)\n",
    "bpr.train(train_set, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we get so far, is the matrix indicating the \"willing\" value of purchasing for a certain user towards a certain item.\n",
    "\n",
    "Hence the shape of the matrix is (#users, #items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bpr = bpr.predictions(range(idx.max()+1))\n",
    "print res_bpr\n",
    "print res_bpr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple performance showcase, this indicates a rough AUC value.\n",
    "\n",
    "While for our recommendation, it is surely higher (above 0.9) since this is a self-learning that is indeed overfitting to historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "bpr_show = BPR(20, idx.max()+1, itm.max()+1)\n",
    "bpr_show.train(train_set[:int(math.floor(len(train_set) * 0.9))], epochs=20)\n",
    "test_set = train_set[int(math.floor(len(train_set) * 0.9)):]\n",
    "bpr_show.test(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Training data according to CustomerID -- making #users as multi-class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_match_dic = dict(idx_match)\n",
    "y_train = data.CustomerID.map(idx_match_dic)\n",
    "# X_train = data[['BUID', 'SKUCode', 'Spending', 'SubDeptCode', 'QTY']]\n",
    "X_train = data[[ 'SKUCode', 'Spending', 'QTY']]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=10.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data are corresponded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data[[ 'SKUCode', 'Spending', 'QTY']]\n",
    "X_test = X_test.groupby('SKUCode').agg({'Spending': lambda x: x.sum(), 'QTY': lambda x: len(x)}) \n",
    "X_test = np.array(X_test.reset_index())\n",
    "sc.fit(X_test)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction result, same format with BPR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lrt = lr.predict_proba(X_test_std) \n",
    "print res_lrt.T\n",
    "print res_lrt.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lr = lr.predict_proba(X_train_std) \n",
    "print res_lr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itm_match_dic = dict(itm_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linear combination of BPR and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is needed before a linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normed_res_bpr = normalize(res_bpr, axis=0, norm='l1')\n",
    "normed_res_lrt = normalize(res_lrt.T, axis=0, norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear model at hand, later can even learn an optimized α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0.5 * normed_res_bpr + 0.5 * normed_res_lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current memory cannot handle such matrix multiplication, thus need to adapt to sparse matrix for matrix manipulating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "purchased = coo_matrix((np.ones(len(itm)),(idx,itm)),shape=(max(idx)+1,max(itm)+1)).tocsr()\n",
    "# purchased = purchased.todense()\n",
    "# willing_from_purchased = np.multiply(res , purchased)\n",
    "# willing_from_unpurchased = res - willing_from_purchased\n",
    "\n",
    "res_csr = coo_matrix(res).tocsr()\n",
    "willing_from_purchased = purchased.multiply(res_csr)\n",
    "willing_from_unpurchased = res_csr - willing_from_purchased\n",
    "\n",
    "# def matrix_reflection(train_set, res):\n",
    "#     res_mat = np.zeros([max(idx)+1,max(itm)+1])\n",
    "#     for each in train_set:\n",
    "#         res_mat[each[0]][each[1]] = res[each[0]][each[1]]\n",
    "#     return res_mat\n",
    "\n",
    "# willing_from_purchased = matrix_reflection(train_set, res)\n",
    "# willing_from_unpurchased = res - willing_from_purchased\n",
    "# from scipy.sparse import coo_matrix\n",
    "# print coo_matrix(willing_from_purchased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating a memory dumping -- no need to use \"Pickle\" -- for coding only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(willing_from_purchased.todense()).to_csv(\"purchased.csv\",header=False)\n",
    "pd.DataFrame(willing_from_unpurchased.todense()).to_csv(\"unpurchased.csv\",header=False)\n",
    "pd.DataFrame(res).to_csv(\"result.csv\",header=False)\n",
    "# read csv to restore memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "## Side Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstmfile = '/Users/Yiteng/venv/rfm-project/data/og_data/Customer_Profile.csv'\n",
    "customer = pd.read_csv(cstmfile, sep=',', engine='c')\n",
    "\n",
    "def strip_brace(input):\n",
    "#     input = input.strip('{')\n",
    "#     input = input.strip('}')\n",
    "    return input[1:-1]\n",
    "#     return input\n",
    "\n",
    "customer.CustomerID = customer.CustomerID.map(strip_brace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use case 1 -- recommend a certain item to users who are interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating dictionaries for content:index matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_match_rev_dic = dict(zip(idx,data.CustomerID))\n",
    "itm_match_rev_dic = dict(zip(itm,data.SKUCode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rec_itm = random.randint(0,itm.max())\n",
    "SKUID_rec_itm = itm_match_rev_dic[rec_itm]\n",
    "print rec_itm, SKUID_rec_itm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a parameter τ here, indicating a % (e.g., 10%) of a selection for compaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1 \n",
    "qualified_no = int(math.floor(res.shape[0]*tau))\n",
    "print 'Recommend item to ' + str(qualified_no) + ' customers out of ' + str(res.shape[0]) + ' in total.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreive the CustomerID to understand the details of this recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product1[product1.SKUID.isin(data.SKUCode)]\n",
    "import heapq\n",
    "rec_col = res[:,rec_itm]\n",
    "nlargest_values = heapq.nlargest(qualified_no, rec_col)\n",
    "rec_customer_indices = [np.where(rec_col == item)[0][0] for item in nlargest_values]\n",
    "rec_customer_ID = map(idx_match_rev_dic.get, rec_customer_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, sub-segment this portion of data out from overall customer data. And following analysis can be based on this data, also can do anything on it based on what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_customer = customer[customer.CustomerID.isin(rec_customer_ID)]\n",
    "print 'Among the total ' + str(sub_customer.shape[0]), 'recommendations, there are ' + str(len(sub_customer[sub_customer.Gender == 'F'])) + ' ladies and ' + str(len(sub_customer[sub_customer.Gender == 'M'])) + ' gentlemen, while ' + str(len(sub_customer[sub_customer.Gender.isnull()])) + ' ppl have not indicated their gender.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'The details of these customers are listed here:'\n",
    "print ''\n",
    "print sub_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use case 2 -- recommend a certain group of users a certain item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a discussion with May-E, this will be based on the original plan of recommending 64 (cube(4)) different user groups based on RFM segmentations.\n",
    "\n",
    "Below are current segmentation according to 4 quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)\n",
    "def RClass(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 2\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)\n",
    "def FMClass(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 4\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 3\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the RFM table according to the data read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "NOW = datetime.date(2017, 1, 1)\n",
    "orders = data\n",
    "orders['tDate2'] = pd.to_datetime(orders['TransactionDate']).dt.date\n",
    "rfmTable = orders.groupby('CustomerID').agg({'tDate2': lambda x: (NOW - x.max()).days, # Recency\n",
    "                                           'TicketNumber': lambda x: len(x),      # Frequency\n",
    "                                           'Spending': lambda x: x.mean()}) # Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmTable['tDate2'] = rfmTable['tDate2'].astype(int)\n",
    "rfmTable.rename(columns={'tDate2': 'recency', \n",
    "                          'TicketNumber': 'frequency', \n",
    "                          'Spending': 'monetary_value'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = rfmTable.quantile(q=[0.25, 0.50, 0.75])\n",
    "quantiles = quantiles.to_dict()\n",
    "rfmSegmentation = rfmTable\n",
    "rfmSegmentation['R_Quartile'] = rfmSegmentation['recency'].apply(RClass, args=('recency',quantiles,))\n",
    "rfmSegmentation['F_Quartile'] = rfmSegmentation['frequency'].apply(FMClass, args=('frequency',quantiles,))\n",
    "rfmSegmentation['M_Quartile'] = rfmSegmentation['monetary_value'].apply(FMClass, args=('monetary_value',quantiles,))\n",
    "rfmSegmentation['RFMClass'] = rfmSegmentation.R_Quartile.map(str) + rfmSegmentation.F_Quartile.map(str) + rfmSegmentation.M_Quartile.map(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentations are ready, hence recommend item based on the result matrix to each segment.\n",
    "\n",
    "One may wonder that why there are many \"recommendation failed\" -- it is because the data contains a vast of missing values (i.e., NaN) for office items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "idx_match_dic = dict(idx_match)\n",
    "\n",
    "for i in range(1,5):\n",
    "    for j in range(1,5):\n",
    "        for k in range(1,5):\n",
    "            # for each RFM Quartile, select the data out from original data based on CustermerID\n",
    "            sub_segment = rfmSegmentation[rfmSegmentation.RFMClass == str(i*100+j*10+k)]\n",
    "            sub_segment = sub_segment.reset_index()\n",
    "\n",
    "            print 'category-R('+str(i)+')-F('+str(j)+')-M('+str(k)+') has '+ str(sub_segment.shape[0]) + ' customers.'\n",
    "\n",
    "            sub_segment_df2 = customer[customer.CustomerID.isin(sub_segment.CustomerID)]\n",
    "            print 'Among them, ' + str(len(sub_segment_df2[sub_segment_df2.Gender == 'F'])) + ' ladies and ' + str(len(sub_segment_df2[sub_segment_df2.Gender == 'M'])) + ' gentlemen, while ' + str(len(sub_segment_df2[sub_segment_df2.Gender.isnull()])) + ' ppl have not indicated their gender.'\n",
    "\n",
    "#             sub_segment_df = orders[orders.CustomerID.isin(sub_segment.CustomerID)]\n",
    "#             some = collections.Counter(sub_segment_df.SKUCode).most_common()\n",
    "#             recommended_pos = int(math.floor(some.__len__()*.4))\n",
    "#             recommended_id = some[recommended_pos][0]\n",
    "\n",
    "            rec_customer_indices = map(idx_match_dic.get, sub_segment.CustomerID)\n",
    "            sub_res = res[rec_customer_indices]\n",
    "            rec_from = sub_res.mean(axis=0)\n",
    "            rec_item_SKUID = itm_match_rev_dic[np.where(rec_from == rec_from.max())[0][0]]            \n",
    "\n",
    "            result = product1[product1.SKUID == rec_item_SKUID] #recommended_id]\n",
    "            try:\n",
    "                if result.shape[0]:\n",
    "                    print 'Recommendation:'+ '    ' + result.iloc[0].DeptName + '    ' + result.iloc[0].SubDeptName\n",
    "                else:\n",
    "                    result = product2[product2.SKUID == recommended_id]\n",
    "                    if result.shape[0]:\n",
    "                        print 'Recommendation:'+ '    ' + result.iloc[0].PRODUCT_ENG_DESC\n",
    "            except:\n",
    "                print 'SKUID ' + str(result.iloc[0].SKUID) + ' is a unknown product -- recommendation failed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
